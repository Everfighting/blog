---
title: AI日报 2026-02-13
date: 2026-02-13 23:59:59
tags:
  - AI日报
  - AI知识点
  - 论文推荐

categories: AI日报
description: 每日AI知识点推送与论文推荐汇总
abbrlink: ai-daily-20260213
toc: true
---

## 📚 今日知识点

1. 【架构】LangChain vs LangGraph：Chain是线性流程，Graph是循环流程。复杂Agent用LangGraph，简单任务用LangChain足够。

2. 【项目经验】成本优化：小模型+Prompt工程 > 大模型+暴力Prompt。7B模型调优后，很多任务能达到GPT-4的80%效果，成本只有1/10。

3. 【方法论】Debug新技巧：把错误信息和相关代码扔给AI，让它在本地环境模拟执行（如果有能力），或分析可能原因。比自己Google快5倍。

4. 【方法论】Debug新技巧：把错误信息和相关代码扔给AI，让它在本地环境模拟执行（如果有能力），或分析可能原因。比自己Google快5倍。

5. 【感悟】与AI协作的三个层次：1.把AI当搜索引擎 2.把AI当工具（写作、编程） 3.把AI当思维伙伴（共创、辩论）。大部分人还停留在第一层。

6. 【方法论】AI辅助读书：读完一章，让AI用费曼技巧给你讲一遍，发现理解偏差。被动阅读变为主动验证。

7. 【Vibe Coding】用自然语言解释需求比写伪代码更有效：AI理解上下文比理解语法更擅长。说清楚'为什么做'比'怎么做'重要。

8. 【Vibe Coding】相信AI但验证结果：让AI写复杂逻辑，但你必须理解代码结构。不理解的东西，永远不要直接部署到生产环境。

9. 【方法论】AI辅助读书：读完一章，让AI用费曼技巧给你讲一遍，发现理解偏差。被动阅读变为主动验证。

10. 【架构】RAG三要素：文档切分（chunk size 512-1024）、向量数据库（FAISS/Pinecone）、召回+重排（BM25+语义检索）。

11. 【架构】Prompt Engineering的层次：1.指令明确 2.提供示例 3.思维链 4.自我反思。大部分问题在前两步就能解决。

12. 【LLM】上下文窗口是记忆容量：GPT-4是128K，Claude是200K。长对话中可以用滑动窗口或RAG技术避免遗忘关键信息。

13. 【感悟】AI时代的竞争力变化：不再是'谁能记住更多'，而是'谁能提出更好问题'和'谁能判断AI答案的质量'。

14. 【架构】LangChain vs LangGraph：Chain是线性流程，Graph是循环流程。复杂Agent用LangGraph，简单任务用LangChain足够。

15. 【感悟】AI不是万能药：它能加速已知路径的探索，但无法替代方向判断。选择什么问题去解决，比如何解决更重要。


## 📖 论文推荐

1. **Constitutional AI (2022)**
   - 作者: Anthropic
   - 简介: 引用2000+！宪法AI，通过自我批评避免有害输出。Claude模型的核心安全机制，RLHF的替代方案。

2. **GPT-2: Language Models are Unsupervised Multitask Learners (2019)**
   - 作者: Radford et al. (OpenAI)
   - 简介: 引用2万+！1.5B参数模型，展示大规模无监督训练的潜力。是GPT-3、ChatGPT的先驱。

3. **Instruction Tuning (2022)**
   - 作者: Wei et al. (Google)
   - 简介: 引用5000+！证明指令微调让单一模型完成多任务。ChatGPT等聊天机器人的技术基础。

4. **Mixture of Agents (2024)**
   - 作者: Together AI
   - 简介: 多Agent协作框架，每个专注一领域。通过aggregation得到更好结果。

5. **Language Models are Few-Shot Learners (2020)**
   - 作者: Brown et al. (OpenAI)
   - 简介: 引用6万+！GPT-3论文，定义few-shot learning。175B参数展示规模化效果，是现代prompt技术的起点。

6. **DeepSeek-V3: Technical Report (2025)**
   - 作者: DeepSeek Team
   - 简介: 开源MoE架构，671B参数激活仅37B。性能媲美GPT-4，训练成本560万美元。开源的里程碑。

7. **o1-preview: OpenAI's Reasoning Model (2024)**
   - 作者: OpenAI
   - 简介: test-time compute首次显式训练，推理前多步思考。数学、编程、科学推理能力大幅提升。

8. **LLM-as-Judge (2024)**
   - 作者: Multiple Teams
   - 简介: 用强模型评估弱模型，自动化评测。降低人工成本，加速模型迭代。

9. **GraphRAG (2024)**
   - 作者: Microsoft Research
   - 简介: 结合知识图谱+社区检测，解决RAG全局性盲点。企业级知识库必备技术。

10. **Chain of Density (2024)**
   - 作者: Various Researchers
   - 简介: 迭代增加摘要密度，简洁且信息丰富。长文档摘要必备。

11. **Mixture of Agents (2024)**
   - 作者: Together AI
   - 简介: 多Agent协作框架，每个专注一领域。通过aggregation得到更好结果。

12. **Emergent Abilities (2022)**
   - 作者: Wei et al. (Google)
   - 简介: 揭示某些能力只在模型规模达到阈值后出现。解释为什么大模型比小模型强很多。

13. **Prompt Caching (2024)**
   - 作者: Anthropic, OpenAI
   - 简介: 缓存prefix重复使用不收费。RAG、长对话可节省50%+成本。生产环境必备优化。

14. **Attention Is All You Need (2017)**
   - 作者: Vaswani et al. (Google Brain)
   - 简介: 引用10万+！Transformer论文，彻底改变NLP领域。自注意力机制是所有大模型（GPT、BERT、Claude）的架构基础。必读经典。

15. **DeepSeek-V3: Technical Report (2025)**
   - 作者: DeepSeek Team
   - 简介: 开源MoE架构，671B参数激活仅37B。性能媲美GPT-4，训练成本560万美元。开源的里程碑。

***

*本日报由AI助手自动生成，帮您快速了解AI领域最新动态。*
